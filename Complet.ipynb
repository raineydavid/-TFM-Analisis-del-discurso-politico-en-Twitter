{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necesario tener importar las siguientes librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "# General:\n",
    "import tweepy           # Para consumir la API de Tweeter\n",
    "import pandas as pd     # Para análisis de datos\n",
    "import numpy as np      # Para cálculo numérico\n",
    "\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "\n",
    "import pickle\n",
    "import pandas as pd \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Embedding, LSTM ,GRU\n",
    "from keras.layers.embeddings import  Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  claves de autenticacion de twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key='CTXchN99VNe7jl8MgZrE47AIm'\n",
    "consumer_secret= 'faQ0b0r6FyfblBIPfJe0k6qjlEEqbynVmghjaKuzZhjskggjFq'\n",
    "access_token = '1118494816105574401-JTGPAQhrSsvFSrd9O0eN03HkFVYjAK'\n",
    "access_secret = 'OWg6aPlumQPSlXFn2kAiQcByNFkWyse0nE9JgXxQlglnR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  funcion de llamada a la api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_setup():\n",
    "    \"\"\"\n",
    "    Función de utilidad para configurar la API de Twitter\n",
    "    con nuestras claves de acceso.\n",
    "    \"\"\"\n",
    "    # Autenticación y acceso usando claves:\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "    # Retornar API con autenticación:\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extraccion de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_caller_csv(id_):\n",
    "        extractor = twitter_setup() \n",
    "        u = extractor.get_user(id_)\n",
    "        private= u.protected\n",
    "\n",
    "        if private == False:\n",
    "            try: \n",
    "                    \n",
    "                    name = u.screen_name  \n",
    "                    tweets = extractor.user_timeline(id=id_, count=200,tweet_mode= 'extended')\n",
    "\n",
    "            except:\n",
    "                    print('Error in try 01: caller')\n",
    "\n",
    "            try:    #CREATE A DATAFRAME   ##02\n",
    "                    alltweet = []\n",
    "                    alltweet.extend(tweets)\n",
    "                    oldest = alltweet[-1].id - 1\n",
    "                    try:\n",
    "                        while len(tweets) > 0:\n",
    "                                tweets = extractor.user_timeline(id= id_,count=200,tweet_mode= 'extended',max_id=oldest)\n",
    "                                alltweet.extend(tweets)\n",
    "                                oldest = alltweet[-1].id - 1\n",
    "                    except ConnectionError:\n",
    "                        print('ConnectionError')\n",
    "                    result = [[tweet.id, tweet.created_at, tweet.full_text, tweet.entities] for tweet in alltweet]\n",
    "                    data= pd.DataFrame(result, columns=['id','date','text', 'entities'])\n",
    "\n",
    "            except:\n",
    "                    print('Error in try: 02, dataframe')\n",
    "\n",
    "            try:    #Extract hashtags     ##03\n",
    "                    hashtags=[]\n",
    "                    for key in data['entities']:\n",
    "                        try:\n",
    "                            hashtags.append(key['hashtags'][0]['text'])\n",
    "                        except: \n",
    "                            hashtags.append('null')\n",
    "                            pass\n",
    "            except: \n",
    "                    print('Error in try: 03, hashtags')\n",
    "\n",
    "\n",
    "            try:    #Extract hashtags     ##04\n",
    "                    menciones = []\n",
    "                    menciones_id = []\n",
    "                    for key in data['entities']:\n",
    "                        try:\n",
    "                            menciones.append(key['user_mentions'][0]['name'])\n",
    "                            menciones_id.append(key['user_mentions'][0]['id'])\n",
    "                        except: \n",
    "                            menciones.append('null')\n",
    "                            menciones_id.append('null')\n",
    "                            pass\n",
    "            except: \n",
    "                    print('Error in try: 04, entities')   \n",
    "\n",
    "\n",
    "            try:    #Extract hashtags     ##05\n",
    "\n",
    "                    retweet_count = [tweet.retweet_count for tweet in alltweet]\n",
    "            except: \n",
    "                    print('Error in try: 05, retweet_count')\n",
    "\n",
    "\n",
    "            try:    #Extract hashtags     ##06\n",
    "\n",
    "                    like = [tweet.favorite_count for tweet in alltweet]\n",
    "            except: \n",
    "                    print('Error in try: 06, likes')\n",
    "\n",
    "\n",
    "\n",
    "            try:    #unique data          ##07\n",
    "                    hashtags= pd.DataFrame(hashtags, columns= ['hashtags'])\n",
    "                    menciones = pd.DataFrame(menciones, columns= ['menciones'])\n",
    "                    menciones_id = pd.DataFrame(menciones_id, columns= ['menciones_id'])\n",
    "                    menciones_combinado = pd.concat([menciones, menciones_id], axis= 1)\n",
    "                    retweet_count = pd.DataFrame(retweet_count, columns= ['numero_retweet'])\n",
    "                    like_count = pd.DataFrame(like, columns=['numero_likes'])\n",
    "\n",
    "            except:\n",
    "                    print('Error in try: 07, dataframes')\n",
    "\n",
    "            try:    #unique data          ##08\n",
    "                    data=pd.concat([data,hashtags, menciones_combinado, retweet_count, like_count], axis= 1)\n",
    "\n",
    "            except:\n",
    "                    print('Error in try: 08, concat')\n",
    "\n",
    "\n",
    "\n",
    "            try:    #drop columna and RT  ##09\n",
    "                data= data.drop(columns='entities')\n",
    "                data = data.drop(data[data.text.str.contains(\"RT\")].index)\n",
    "\n",
    "            except:\n",
    "                    print('Error in try: 09, drop') \n",
    "\n",
    "            data.to_csv(name+'.csv', line_terminator='\\rn')\n",
    "        else:\n",
    "            print('private user')\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraccion de followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor_follows(id_):\n",
    "    extractor= twitter_setup()\n",
    "    ids = []\n",
    "    u = extractor.get_user(id_)\n",
    "    name = u.screen_name\n",
    "    try:\n",
    "        for page in tweepy.Cursor(extractor.followers_ids, id=id_).pages():\n",
    "            ids.extend(page)\n",
    "    except: ConnectionError      \n",
    "    ids=pd.DataFrame(ids,columns=[name],dtype=object)\n",
    "    return ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor_follows_01(id_):\n",
    "    extractor= twitter_setup()\n",
    "    ids = []\n",
    "    u = extractor.get_user(id_)\n",
    "    name = u.screen_name\n",
    "    try:\n",
    "        for page in tweepy.Cursor(extractor.followers_ids, id=id_).pages():\n",
    "            ids.extend(page)\n",
    "    except: ConnectionError      \n",
    "    \n",
    "    return ids, name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_of_followers(id_):\n",
    "    \n",
    "        extractor= twitter_setup()\n",
    "        \n",
    "        df = extractor_follows(id_)\n",
    "        u = extractor.get_user(id_)\n",
    "        private= u.protected\n",
    "        \n",
    "        if private== False:\n",
    "            name_user = u.screen_name\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                followers=[]\n",
    "                names= []\n",
    "                for i in df[name_user]:\n",
    "                    u = extractor.get_user(i)\n",
    "                    private= u.protected\n",
    "                    \n",
    "                    if private == False:\n",
    "                    \n",
    "                        follower, name= extractor_follows_01(i)\n",
    "                        followers.append(follower)\n",
    "                        names.append(i)\n",
    "                                      \n",
    "                    else:\n",
    "                        print('error')\n",
    "                        pass\n",
    "                        \n",
    "                followers_dt= pd.DataFrame(followers,dtype=object).transpose()\n",
    "                names_series= pd.Series(names)\n",
    "                followers_dt=followers_dt.rename(columns=names_series)\n",
    "                grid= pd.concat([df,followers_dt],axis=1)\n",
    "                grid.to_csv(name_user +'grid_of_follows.csv',mode= 'a')\n",
    "            except:\n",
    "                print('error 2')\n",
    "                pass\n",
    "            \n",
    "        else:\n",
    "            print('private user')\n",
    "            pass\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcion prediccion de sentimiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediciones_keras(tweets):\n",
    "    keras_tweet = pickle.load(open('model_predict_keras.sav', 'rb'))\n",
    "    dt= pd.read_csv('train_tweet.csv')\n",
    "    sentences= dt['content'].values\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    word_index= tokenizer.word_index\n",
    "    \n",
    "    tweet_01=[]\n",
    "    for i in list(tweets):\n",
    "        try:\n",
    "            lista= []\n",
    "            a=i.split()\n",
    "            a= list(a)\n",
    "            for v in a:\n",
    "                lista.append(v.encode('ascii', 'ignore').decode('ascii'))\n",
    "            tweet_01.append(lista)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    tweet_02=[]\n",
    "    for i in tweet_01:\n",
    "        lista=[]\n",
    "        for line in i:\n",
    "            tokens = word_tokenize(line)\n",
    "            tokens = [w.lower() for w in tokens]\n",
    "            table = str.maketrans('','',string.punctuation)\n",
    "            stripped = [w.translate(table) for w in tokens]\n",
    "            words= [word for word in stripped if word.isalpha()]\n",
    "            stop_words = set(stopwords.words('spanish'))\n",
    "            words= [w for w in words if not w in stop_words]\n",
    "            lista.append(words)\n",
    "        tweet_02.append(lista)  \n",
    "        \n",
    "    predictions_mean=[]\n",
    "    for i in tweet_02:\n",
    "        try:\n",
    "            predictions= tokenizer.texts_to_sequences(i)\n",
    "            predictions= pad_sequences(predictions, padding= 'post', maxlen= 30)\n",
    "            predictions_mean.append(keras_tweet.predict(predictions).mean())\n",
    "        except:\n",
    "            pass\n",
    "    global_predicition= sum(predictions_mean)/len(predictions_mean)\n",
    "    return global_predicition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construccion del Grafico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1): \n",
    "  \n",
    "    \n",
    "    unique_list = [] \n",
    "        \n",
    "    for x in list1: \n",
    "        \n",
    "        if x not in unique_list and x!= 0: \n",
    "            unique_list.append(x) \n",
    "    \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_network(dt):\n",
    "    dt = dt.drop(columns='Unnamed: 0')\n",
    "    dt = dt.fillna(0)\n",
    "    name= dt.columns[0]\n",
    "    df_list = []\n",
    "    for col in dt:\n",
    "        for v in dt[col]:\n",
    "            df_list.append(v)\n",
    "            \n",
    "    nodes_02 = unique(df_list)\n",
    "    \n",
    "    edges_02= []\n",
    "    for col in dt:\n",
    "        for v in dt[col]:\n",
    "            if v != 0:\n",
    "                edges_02.append([col,v])\n",
    "    edges_03=[]\n",
    "    for i in edges_02:\n",
    "        edges_03.append(tuple(i))\n",
    "    \n",
    "    g= Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    g.add_node(name,label=name,color='red',)\n",
    "\n",
    "    for i in nodes_02:\n",
    "        g.add_node(i, label= i)\n",
    "\n",
    "    g.add_edges(edges_03)\n",
    "    g.show(name+'grahp.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_graph_network(dt):\n",
    "    dt = dt.drop(columns='Unnamed: 0')\n",
    "    dt = dt.fillna(0)\n",
    "    name= dt.columns[0]\n",
    "    df_list = []\n",
    "    for col in dt:\n",
    "        for v in dt[col]:\n",
    "            df_list.append(v)\n",
    "            \n",
    "    nodes_02 = unique(df_list)\n",
    "    \n",
    "    edges_02= []\n",
    "    for col in dt:\n",
    "        for v in dt[col]:\n",
    "            if v != 0:\n",
    "                edges_02.append([col,v])\n",
    "    edges_03=[]\n",
    "    for i in edges_02:\n",
    "        edges_03.append(tuple(i))\n",
    "    \n",
    "    dicts= {}\n",
    "    for i in nodes_02:\n",
    "        extractor= twitter_setup()\n",
    "        if  i== name:\n",
    "            try:\n",
    "                    u=extractor.get_user(name)\n",
    "                    id_= u.id\n",
    "                    tweet_caller_csv(id_)\n",
    "                    tweets = pd.read_csv(name+'.csv', dtype=object)\n",
    "                    tweets= tweets['text']\n",
    "                    dicts[i]= prediciones_keras(tweets)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            \n",
    "            try:\n",
    "                u = extractor.get_user(i)\n",
    "                name = u.screen_name\n",
    "                tweet_caller_csv(i)\n",
    "                tweets = pd.read_csv(name+'.csv', dtype=object)\n",
    "                tweets= tweets['text']\n",
    "                dicts[i]= prediciones_keras(tweets)\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    for i in dicts:\n",
    "        if dicts.get(i)>0.25:\n",
    "            dicts[i]= 'red'\n",
    "        else:\n",
    "            dicts[i]= 'blue'  \n",
    "        \n",
    "    g= Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    g.add_node(name,label=name,color='green')\n",
    "\n",
    "    for i in dicts:\n",
    "        g.add_node(i,lable= i,color=dicts.get(i))\n",
    "\n",
    "    g.add_edges(edges_03)\n",
    "    g.show(name+'grahp.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"%%time\n",
    "tweet_caller_csv(924114887722364929)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"%%time\n",
    "grid_of_followers(924114887722364929)\n",
    "Wall time: 9.45 s\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt = pd.read_csv('Carlos0520683grid_of_follows.csv',dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph_network(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tweets = pd.read_csv('Carlos0520683.csv', dtype=object)\n",
    "tweets= tweets['text']\n",
    "\n",
    "prediciones_keras(tweets)\n",
    "output= 0.3338531681469509'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dt = pd.read_csv('Carlos0520683grid_of_follows.csv',dtype=object)\n",
    "sentiment_graph_network(dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
